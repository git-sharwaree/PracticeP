{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ada300d-d602-4ad2-8beb-2719759ef0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Set the Java home\n",
    "os.environ['JAVA_HOME'] = r'C:\\Program Files\\OpenLogic\\jdk-11.0.24.8-hotspot'\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('practice') \\\n",
    "    .config('spark.jars.packages', 'org.apache.spark:spark-sql_2.12:3.3.1') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7288fd-4425-414b-bb74-e01630ed36fe",
   "metadata": {},
   "source": [
    "#### when running in the local you have one master node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c14b13e1-8705-45b4-afb3-d4a1e471bcb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-MTJATEQ:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>practice</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2efe8a36030>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d831916-8978-4bc2-9a5f-2a990afeace4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r'C:\\Users\\Admin\\Desktop\\testdata.csv'\n",
    "df_pyspark=spark.read.csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "153a067b-fb56-426f-8007-bc94228dd072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[_c0: string, _c1: string, _c2: string]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c63bb3-9edb-4851-8867-8ca8087e056e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|      _c0|_c1|       _c2|\n",
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "|Sharwaree| 20|        10|\n",
      "|  Sarthak| 17|         8|\n",
      "|   Advika| 10|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad35249-4456-45cc-ada4-65f5fc424fc2",
   "metadata": {},
   "source": [
    " here we have changed the default column values of c0 and c1 to actual vals name & age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e7f5f97-338a-48b9-91c6-717e45e493f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|Sharwaree| 20|        10|\n",
      "|  Sarthak| 17|         8|\n",
      "|   Advika| 10|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv(file_path)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e9d2841-088b-4860-a441-af2af65e3129",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "efff32b8-7923-4004-9e10-c9df3532ab89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Name='Sharwaree', Age='20', Experience='10'),\n",
       " Row(Name='Sarthak', Age='17', Experience='8'),\n",
       " Row(Name='Advika', Age='10', Experience='4')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f24e19-9b9c-4f63-8f4a-56c24f85f9fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#similar to df.info() \n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7f616f47-8d8b-4de2-9253-dcd2b089bf08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to remove the default option of string use inferschema, now the datatype of age is integer\n",
    "df_pyspark=spark.read.option('header','true').csv(file_path,inferSchema=True)\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ccb811-1609-461a-8c80-48301474b261",
   "metadata": {},
   "source": [
    "alternate way of reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "572e4780-9302-4986-9cb7-a29c61fa38dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|Sharwaree| 20|        10|\n",
      "|  Sarthak| 17|         8|\n",
      "|   Advika| 10|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark=spark.read.csv(file_path,header=True,inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d2891ec7-e566-4568-adb6-94af36f587c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a143562c-5965-4256-abad-5a9c923d6562",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#column selection\n",
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ede9acf1-f83e-41e0-bc15-e56a207c5af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+\n",
      "|     Name|Experience|\n",
      "+---------+----------+\n",
      "|Sharwaree|        10|\n",
      "|  Sarthak|         8|\n",
      "|   Advika|         4|\n",
      "+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2d2811c6-74c9-4624-bc62-6c63cc979132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Name', 'string'), ('Age', 'int'), ('Experience', 'int')]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e3565843-fde5-401d-966a-a74baed75c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+------------------+-----------------+\n",
      "|summary|     Name|               Age|       Experience|\n",
      "+-------+---------+------------------+-----------------+\n",
      "|  count|        3|                 3|                3|\n",
      "|   mean|     NULL|15.666666666666666|7.333333333333333|\n",
      "| stddev|     NULL| 5.131601439446884|3.055050463303893|\n",
      "|    min|   Advika|                10|                4|\n",
      "|    max|Sharwaree|                20|               10|\n",
      "+-------+---------+------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54286b10-282e-4051-b0e2-fef044b6c241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+----------------------+\n",
      "|     Name|Age|Experience|Experience after 2 yrs|\n",
      "+---------+---+----------+----------------------+\n",
      "|Sharwaree| 20|        10|                    12|\n",
      "|  Sarthak| 17|         8|                    10|\n",
      "|   Advika| 10|         4|                     6|\n",
      "+---------+---+----------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#adding colms , not an inplace op \n",
    "df_pyspark=df_pyspark.withColumn('Experience after 2 yrs',df_pyspark['Experience']+2)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "60799b73-56d4-4cf0-9f65-f35e2c31bab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|Sharwaree| 20|        10|\n",
      "|  Sarthak| 17|         8|\n",
      "|   Advika| 10|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## dropping colmns\n",
    "df_pyspark.drop('Experience after 2 yrs').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91379ef-e4ab-4eba-a8f3-5d81dbd3d5d4",
   "metadata": {},
   "source": [
    "drop is not an inplace op so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2511fa38-e4dc-4c96-8cb7-2cdd49a0d4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.drop('Experience after 2 yrs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1052c6dd-9681-48d3-b851-2c0134c8fc7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     Name|Age|Experience|\n",
      "+---------+---+----------+\n",
      "|Sharwaree| 20|        10|\n",
      "|  Sarthak| 17|         8|\n",
      "|   Advika| 10|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "22c6a2dc-5e6f-49d0-8376-11014076576d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+---+\n",
      "|     Name|Age|Exp|\n",
      "+---------+---+---+\n",
      "|Sharwaree| 20| 10|\n",
      "|  Sarthak| 17|  8|\n",
      "|   Advika| 10|  4|\n",
      "+---------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Renaming the colm\n",
    "df_pyspark.withColumnRenamed('Experience','Exp').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3faf5c6c-3eda-439d-b729-f7239adafac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| Name| Age|Salary|\n",
      "+-----+----+------+\n",
      "| Sara|  20| 30000|\n",
      "| Arya|  20| 60000|\n",
      "|manas|  20|  NULL|\n",
      "| NULL|NULL|  NULL|\n",
      "| Ashi|NULL| 40000|\n",
      "|Tashi|NULL|  NULL|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath=r'C:\\Users\\Admin\\Desktop\\test2.csv'\n",
    "df=spark.read.csv(filepath,header=True,inferSchema=True)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98ee180e-a479-427d-b514-d505f7bb3caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|Name|Age|Salary|\n",
      "+----+---+------+\n",
      "|Sara| 20| 30000|\n",
      "|Arya| 20| 60000|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#dropping null vals \n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c07aa641-4c4d-4f15-a89a-8be469e6df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|Name|Age|Salary|\n",
      "+----+---+------+\n",
      "|Sara| 20| 30000|\n",
      "|Arya| 20| 60000|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"any\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5433fd5e-6f8b-45bd-9f8e-e2ee118edce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| Name| Age|Salary|\n",
      "+-----+----+------+\n",
      "| Sara|  20| 30000|\n",
      "| Arya|  20| 60000|\n",
      "|manas|  20|  NULL|\n",
      "| Ashi|NULL| 40000|\n",
      "|Tashi|NULL|  NULL|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.na.drop(how=\"all\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8bd6dc18-f322-4955-afb9-83fb0d18c277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+\n",
      "| Name| Age|Salary|\n",
      "+-----+----+------+\n",
      "| Sara|  20| 30000|\n",
      "| Arya|  20| 60000|\n",
      "|manas|  20|  NULL|\n",
      "| Ashi|NULL| 40000|\n",
      "+-----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold if thresh=2 display all rows with atleast 2 non null vals \n",
    "df.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "047f2a82-dcd7-4802-83e2-d16da02eb367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|Name| Age|Salary|\n",
      "+----+----+------+\n",
      "|Sara|  20| 30000|\n",
      "|Arya|  20| 60000|\n",
      "|Ashi|NULL| 40000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#subset -delete null vals only from salary colm\n",
    "df.na.drop(how=\"any\",subset=['Salary']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "bc0e828d-e60b-4e2b-b9a7-1fa463669644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+\n",
      "|        Name| Age|Salary|\n",
      "+------------+----+------+\n",
      "|        Sara|  20| 30000|\n",
      "|        Arya|  20| 60000|\n",
      "|       manas|  20|  NULL|\n",
      "|missing vals|NULL|  NULL|\n",
      "|        Ashi|NULL| 40000|\n",
      "|       Tashi|NULL|  NULL|\n",
      "+------------+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill (val,subset=None)\n",
    "#string vals\n",
    "df=df.na.fill('missing vals')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9022e7e-f3e7-47ef-b274-a478529a250d",
   "metadata": {},
   "source": [
    "#for integer\n",
    "#here only age col will be replaced with 0 salary will still have null vals\n",
    "df=df.na.fill(0,subset=['Age'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6a0675ae-44da-41a4-9800-f1f365e316a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing with mean\n",
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age','Salary'],\n",
    "    outputCols=[\"{}_imputed\".format(c) for c in ['Age','Salary']]).setStrategy(\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1e8cdf5d-2cf9-4767-86c9-c2b852121894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----+------+-----------+--------------+\n",
      "|        Name| Age|Salary|Age_imputed|Salary_imputed|\n",
      "+------------+----+------+-----------+--------------+\n",
      "|        Sara|  20| 30000|         20|         30000|\n",
      "|        Arya|  20| 60000|         20|         60000|\n",
      "|       manas|  20|  NULL|         20|         43333|\n",
      "|missing vals|NULL|  NULL|         20|         43333|\n",
      "|        Ashi|NULL| 40000|         20|         40000|\n",
      "|       Tashi|NULL|  NULL|         20|         43333|\n",
      "+------------+----+------+-----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df).transform(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ede7e2-c423-4771-939c-f67a4cbe4b0d",
   "metadata": {},
   "source": [
    "#### filter operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1925c886-005c-46d7-94ce-898d32dcbd14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|Name| Age|Salary|\n",
      "+----+----+------+\n",
      "|Sara|  20| 30000|\n",
      "|Ashi|NULL| 40000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Salary<=40000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "338e2fa2-5ea4-49a3-a124-3fcd044c6af0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+\n",
      "|Name|\n",
      "+----+\n",
      "|Arya|\n",
      "+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(\"Salary>50000\").select(['Name']).show() #select can take multiple cols "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "024d967e-5bb5-451e-9f23-4cbe2b565dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+------+\n",
      "|Name| Age|Salary|\n",
      "+----+----+------+\n",
      "|Ashi|NULL| 40000|\n",
      "+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#multiple filters using and,or ..\n",
    "df.filter((df['Salary']>30000) &\n",
    "          (df['Salary']<60000)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5307644a-cb55-4d6b-acfa-2d6e505fc0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+------+\n",
      "|Name|Age|Salary|\n",
      "+----+---+------+\n",
      "|Sara| 20| 30000|\n",
      "|Arya| 20| 60000|\n",
      "+----+---+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~((df['Salary']>30000) & (df['Salary']<60000))).show() #salary greater than 30000 and less than 60000 is 40000 so\n",
    "#implying not,it gives sara,arya"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af16cd1-bbbd-4917-954a-ade4f3e83ed7",
   "metadata": {},
   "source": [
    "###### GroupBy and Aggregate Fucns(Pre-processing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "6c81e103-4d62-4bb4-8900-33c66fd8fbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----------+-------+\n",
      "| Name|Department| Salary|\n",
      "+-----+----------+-------+\n",
      "| Kaya|  Data Sci|1000000|\n",
      "| Kaya|       IOT| 100000|\n",
      "|Arnav|  Data Sci| 850000|\n",
      "|Kunal|       IOT| 700000|\n",
      "|Kunal|      ENTC| 600000|\n",
      "|Arnav| Big Data | 950000|\n",
      "|Arnav|       IOT| 750000|\n",
      "| Kaya| Big Data | 900000|\n",
      "+-----+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file=r'C:\\Users\\Admin\\Desktop\\test3.csv'\n",
    "df_o=spark.read.csv(file,header=True,inferSchema=True)\n",
    "df_o.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "db3af5ed-b42c-4aea-9ef2-0877afe71ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|sum(Salary)|\n",
      "+-----+-----------+\n",
      "|Kunal|    1300000|\n",
      "|Arnav|    2550000|\n",
      "| Kaya|    2000000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#group to find max salary\n",
    "df_o.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bfd66601-5261-47f8-a25f-9b78221599e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+\n",
      "| Name|max(Salary)|\n",
      "+-----+-----------+\n",
      "|Kunal|     700000|\n",
      "|Arnav|     950000|\n",
      "| Kaya|    1000000|\n",
      "+-----+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_o.groupBy('Name').max().show()  #kunal's highest salary is 700000 , kaya's highest sal is 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "937ea030-0741-44d2-9ee9-fea9dfbee622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+\n",
      "|Department|sum(Salary)|\n",
      "+----------+-----------+\n",
      "|       IOT|    1550000|\n",
      "| Big Data |    1850000|\n",
      "|  Data Sci|    1850000|\n",
      "|      ENTC|     600000|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#which department gives max salary\n",
    "df_o.groupBy('Department').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "954449ce-91a0-435c-8413-af6526bddaae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------+\n",
      "|Department|      avg(Salary)|\n",
      "+----------+-----------------+\n",
      "|       IOT|516666.6666666667|\n",
      "| Big Data |         925000.0|\n",
      "|  Data Sci|         925000.0|\n",
      "|      ENTC|         600000.0|\n",
      "+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_o.groupBy('Department').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ec498c26-841f-4ed5-839b-63f1349aa25f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "|       IOT|    3|\n",
      "| Big Data |    2|\n",
      "|  Data Sci|    2|\n",
      "|      ENTC|    1|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#num of people in each department\n",
    "df_o.groupBy('Department').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6cd82a38-3e22-4cbc-9922-a2a8c0dab9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|sum(Salary)|\n",
      "+-----------+\n",
      "|    5850000|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_o.agg({'Salary':'sum'}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e91954-3f5e-48d9-9b4b-a375d75afc54",
   "metadata": {},
   "source": [
    "#### ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "dd424c8d-08ee-4b0d-9ebe-38f6d143b805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+\n",
      "|    Name|Age|Experience|Salary|\n",
      "+--------+---+----------+------+\n",
      "|   Aryan| 31|        10| 30000|\n",
      "|   Dhruv| 30|         8| 25000|\n",
      "|   Ishan| 29|         4| 20000|\n",
      "| Natasha| 24|         3| 20000|\n",
      "|    Ojas| 21|         1| 15000|\n",
      "|Shivanya| 23|         2| 18000|\n",
      "+--------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "path=r'C:\\Users\\Admin\\Desktop\\test4.csv'\n",
    "dfr=spark.read.csv(path,header=True,inferSchema=True)\n",
    "dfr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c103b842-013b-4caf-95a6-51bc979e06f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Experience: integer (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfr.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "17ffddba-aa92-45e3-980e-7571ef26105a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfr.columns #list of cols . columns is not a func/method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "86c93bc8-b922-4261-91e9-c8db89437078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "featureassembler=VectorAssembler(inputCols=[\"Age\",\"Experience\"],outputCol=\"Independent features\")\n",
    "                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fcbefe2e-cfea-4404-9e6a-cc5140bd02db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+------+--------------------+\n",
      "|    Name|Age|Experience|Salary|Independent features|\n",
      "+--------+---+----------+------+--------------------+\n",
      "|   Aryan| 31|        10| 30000|         [31.0,10.0]|\n",
      "|   Dhruv| 30|         8| 25000|          [30.0,8.0]|\n",
      "|   Ishan| 29|         4| 20000|          [29.0,4.0]|\n",
      "| Natasha| 24|         3| 20000|          [24.0,3.0]|\n",
      "|    Ojas| 21|         1| 15000|          [21.0,1.0]|\n",
      "|Shivanya| 23|         2| 18000|          [23.0,2.0]|\n",
      "+--------+---+----------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#take the DataFrame dfr and apply the VectorAssembler transformation\n",
    "output=featureassembler.transform(dfr)\n",
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "bb6164ee-31d7-4dab-8e25-b8e99799e417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name', 'Age', 'Experience', 'Salary', 'Independent features']"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "b4937638-558a-419a-97f8-b4092b719c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|Independent features|Salary|\n",
      "+--------------------+------+\n",
      "|         [31.0,10.0]| 30000|\n",
      "|          [30.0,8.0]| 25000|\n",
      "|          [29.0,4.0]| 20000|\n",
      "|          [24.0,3.0]| 20000|\n",
      "|          [21.0,1.0]| 15000|\n",
      "|          [23.0,2.0]| 18000|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fi_data=output.select(\"Independent features\",\"Salary\")\n",
    "fi_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "a2816e49-8bf5-4bfa-92df-6742e6a72658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "train_data,test_data=fi_data.randomSplit([0.75,0.25])\n",
    "regressor=LinearRegression(featuresCol='Independent features',labelCol='Salary')\n",
    "regressor=regressor.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4c476716-dd0e-4180-a215-8f995c483272",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-383.9733, 1711.1853])"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "81dadbc7-5a72-4c0d-8c26-ff6314d7be30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23998.330550919385"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regressor.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "dcda4ebf-9204-4aaa-8ea2-0d16cd248623",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict=regressor.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f50b57ce-97c3-419e-bf73-1b7e92824364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------------------+\n",
      "|Independent features|Salary|        prediction|\n",
      "+--------------------+------+------------------+\n",
      "|          [21.0,1.0]| 15000| 17646.07679465793|\n",
      "|          [23.0,2.0]| 18000|18589.315525876566|\n",
      "+--------------------+------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predict.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b43d0-73f7-4d24-ae88-87e8a8a40203",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYSPARK_KERNEL",
   "language": "python",
   "name": "pyspark_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
